# Thomas Sterling, Daniel Kogler, Matthew Anderson, et al. SLOWER: A performance model for Exascale computing

SLOWER defines a six-dimensional design trade-off space based on sources
of performance degradation that are invariant across system classes.
(SLOWER定义了6个维度的

An experimental execution model, ParalleX, is described to postulate one
possible advanced abstraction upon which to base next generation
hardware and software systems.

#  Hartmut Kaiser et al. ParalleX: An Advanced Parallel Execution Model for Scaling- Impaired Applications.

并行化应用程序不能很好利用数百个处理器，即scalaribility

These codes exploit the efficiencies of Adaptive Mesh Refinement (AMR)
algorithms to concentrate processing effort at the most active parts of
the computation space at any one time. However, conventional parallel
programming methods using MPI and systems such as distributed memory
MPPs and Linux clusters exhibit poor efficiency and constrained
scalability, severely limiting scientific advancement.

To achieve dramatic improvements for such problems and prepare them for
exploitation of [Peta-flops]{.mark} systems comprising [millions of
cores,]{.mark} a new execution model and programming methodology is
required -- [ParallelX model]{.mark}

Future applications like [AMR algorithms]{.mark} will involve the
processing of large [time-varying graphs with embedded
meta-data.]{.mark}
(AMR模型的网格结构不断调整变化，网格图随时间变化，元数据结构也随之变化，ParallelX模型能提高并行效率)

[新的瓶颈（SLOW）：]{.mark}

-   *Starvation* -- due to lack of usable application parallelism and
    means of managing it,

-   *Overhead* -- reduction to permit strong scalability, improve
    efficiency, and enable dynamic resource management,

-   *Latency* -- from remote access across system or to local memories,

-   *Contention* -- due to multicore chip I/O pins, memory banks, and
    system interconnects.

[或者是：]{.mark}

-   Starvation occurs when there is insufficient concurrent work
    available to maintain high utilization of all resources.

-   Latencies are imposed by the time-distance delay intrinsic to
    accessing remote resources and services.

-   Overhead is work required for the management of parallel actions and
    resources on the critical execution path, which is not necessary in
    a sequential variant.

-   Waiting for contention resolution is the delay due to the lack of
    availability of oversubscribed shared resources.

The ParalleX model has been devised to address these challenges by
enabling a new computing dynamic through the application of
[message-driven computation]{.mark} in a [global address space
context]{.mark} with lightweight synchronization.

## 并行执行的ParallelX模型(parallex model of parallel execution)

ParalleX will improve efficiency by reducing average synchronization and
scheduling over-head, improve utilization through asynchrony of work

Global barriers are essentially eliminated as the principal means of
synchronization, and instead replaced by lightweight [Local Control
Objects (LCOs)]{.mark} that can be used for a plethora of purposes from
simple mutex co-structs to sophisticated *futures* semantics for
anonymous producer-consumer operation.

ParalleX exhibits an *active global address space* (AGAS) that is an
extension of experimental [partitioned global address space]{.mark}
(PGAS).

## 5. 试验

[Fibonacci计算：]{.mark}

![](./media/image1.emf){width="3.273670166229221in"
height="2.0315463692038493in"}

[HPX开发AMR应用]{.mark}：Octopus: an HPX octree-based 3D AMR framework

各数据点实施为独立的dataflow模板，有3个输入和3个输出，其中输出连接到临近数据点的输出，在各时间步完成后，在数据点之间完成需要的数据转移。图3展示数据布局的示意。该方法回避了[全局barrier]{.mark}，因为各数据点在其相邻的之前数据点计算时继续计算。

![](./media/image2.emf){width="3.2012007874015747in"
height="3.702194881889764in"}

# U.S. Wickramasinghe et al. Evaluating Collectives in Networks of Multicore/Two-level Reduction

[本文对比分析MPI +OpenMP和AMT (HPX-5)实施的Lulesh]{.mark}

多核电脑集群的HPC通常采用MPI+X的方式。一些操作，如reduction（规约），这种混合方式涉及scalability-limiting的独立并行操作序列。例如MPI+OpenMP通常要[执行全局并行规约]{.mark}：（1）首先执行一个局部OpenMP规约；（2）再执行跨节点的MPI规约。如果局部规约不是很好的well-balanced，诸如不规则数据或AMR场景，整体的规约操作的scalability很受限制。本文研究不平衡规约对2种执行模型的影响：MPI+OpenMP和AMT。探讨了几种方法：MPI+OpenMP
(使用OpenMP tasking)、仅使用MPI。

[Lulesh
mini-app]{.mark}，尽管使用MPI+OpenMP的异步功能，但还是发现随着尺度和噪声增大，与AMT模型比，MPI+X模型的尺度化显著下降。

## 1引言

AMT（Asynchronous many-task systems (AMT),
异步运行时系统）可替代MPI+X模式，AMT通过使用大量轻量级线程挖掘最大的并行性。AMT有：[Charm++,
OCR, HPX, Legion]{.mark}，均支持共享式和分布式内存并行。AMT尚在研究中。

L. Kale and S. Krishnan, "CHARM++: A Portable Concurrent Object Oriented
System Based on C++" in Proceedings of OOPSLA'93, A. Paepcke, Ed. ACM
Press, September 1993, pp. 91-108.

T. Mattson, R. Cledat, V. Z. Budimlic, Cave, S. Chatterjee, B.
Seshasayee, R. van der Wijngaart, and V. Sarkar., "OCR the open
community runtime interface, version 1.0.0," June 2015.

M. Bauer, S. Treichler, E. Slaughter, and A. Aiken, "[Legion: Expressing
locality and independence with logical regions]{.mark}," in Proceedings
of SC 2012. Los Alamitos, CA, USA: IEEE Computer Society Press, 2012,
pp. 66:1--66:11. \[Online\]. Available:
http://dl.acm.org/citation.cfm?id=2388996.2389086

## 研究背景

[典型的MPI+OpenMPI模型]{.mark}中的reduction
barrier的执行模式：使用fork-join模式的严格顺序执行的局部规约阶段，接着就是：诸如collective操作的全局同步原语。

![](./media/image3.emf){width="4.058569553805774in"
height="1.9675776465441819in"}

图1展示了这种限制。数据流图描述了独立区域A和C（没有直接联系），区域B，D是依赖的。区域B依赖区域A，然后区域D依赖区域B和C。对于不规则荷载情况，重叠区域C和B非常有益。但是，隐式的同步barrier是限制因子，不能隐藏区域A中的不规则性。原始的MPI+OpenMP很难完全利用计算资源（具有并行的数据依赖特性）。新的OpenMP
3.0解决该问题，嵌入动态循环调度和任务并行化技术，例如编程结构#pragma omp
task, #pragma omp sections和nested
regions，但这增加软件复杂度，且性能调优困难。

![](./media/image4.emf){width="4.013888888888889in"
height="1.9845111548556431in"}

[AMT模式]{.mark}：提供统一的集合(collective)方法，甚至是不均衡荷载情况，这得益于AMT的异步设计。如图1，AMT可有效重叠域（A，B）的计算与通信，将他们与区域C联合，因此避免同步步骤的等待时间，增加输出。Threads
can compensate for late comers by taking up more work while waiting for
a collective communication operation to complete.

## HPX-5

HPX-5应用拓展共享内存程序，其中线程显式地发送active messages到global
addresses
(GAS)，这时它们变为新的轻量级线程。这些线程可以成块地形成全局分配的局部同步对象（即future,
dataflow等），来实现控制和数据同步，也可以直接使用GAS实施非阻塞内存转移(puts
and gets)。

HPX-5对[局部轻量级线程]{.mark}实施传统的work stealing scheduler, a high
performance Partitioned GAS (PGAS) for active message addressing and
RDMA operations, and uses the Photon RDMA library for network transport.

HPX-5 implemented a [non-blocking collectives interface]{.mark} that
operates at the lightweight thread level. [As with MPI, threads
interact]{.mark} with the collective through two phases, first joining
the collective and then later testing the collective for completion.
This allows threads to [overlap collective communication with
computation and tolerate latency and irregularity.]{.mark}

allreduce

### A. A Collective Communication Framework {#a.-a-collective-communication-framework .标题3}

### B. Network Collective Implementation {#b.-network-collective-implementation .标题3}

## 模拟荷载不均衡

![](./media/image5.emf){width="4.252334864391951in"
height="2.4024081364829395in"}

# Hartmut Kaiser et al. HPX -- A Task Based Programming Model in a [Global Address Space]{.mark}. 2014

## 1 引言

新的并行化方法应强调以下属性：

Scalability -- enable applications to strongly scale to Exascale levels
of parallelism;

Programmability -- clearly reduce the burden we are placing on high
performance programmers;

Performance Portability-- eliminate or significantly minimize
requirements for porting to future platforms;

Resilience -- properly manage fault detection and recovery at all
components of the software stack;

Energy Efficiency -- maximally exploit dynamic energy saving
opportunities, leveraging the tradeoffs between energy efficiency,
resilience, and performance.

[Intra-node (OpenMP, CUDA)]{.mark}：Functions expected from novel,
self-aware, resilient runtime systems are the autonomic, runtime-dynamic
management of resources, dynamic load balancing, intrinsic latency
hiding mechanisms, management of data movement and locality, active
power management, and detection and recovery from faults

[Inter-node (MPI +X)]{.mark}: the demand for homogeneous programming
interfaces which simplify code development and abstract hardware
specifics additionally requires computer scientist to rethink the way
inter-node parallelism is exposed to the programmers.

[联合global adree space 与task based
parallelism]{.mark}，重新思考并行化，着眼于可编程性(programmability)，代替message
passing。使用active component扩展[Partitioned Global Address Space
(PGAS)]{.mark}概念，获得[Active Global Address
Space]{.mark}，扩展实现了支持有效的和动态的通信、同步、调度和任务定位和数据迁移，还有the
autonomic management of resources, identifying and reacting to load
imbalances and the intermittent loss of resources

## 2. THE STATE OF THE ART

Task based parallel programming models分类：

-   Library solutions：TBB, PPL, Qthreads

-   Language extensions：Intel Cilk Plus, OpenMP 3.0/4.0

-   Experimental programming languages: Chapel, Charm++, X10

上述方法，一些使用futures-based编程模型，有一些使用dataflow处理依赖task（显示和隐式使用控制流的DAG表示）。大多数task-based编程模型处理Node层并行化，HPX提供[统一执行远程和局部操作]{.mark}(homogeneous
execution of remote and local operations)的解决方法。

[编程语言]{.mark}方面：OpenMP
3.0/4.0是FORTRAN语言[唯一的]{.mark}task-based编程模型，OpenMP
4.0引入task dependencies,
着眼于fork-join编程。基于原语的运行时库的一个优点是：支持加速器，如OpenACC,
OpenMP 5.0. 加速器CUDA/OpenCL

Intel TBB是C++语言，提供任务的codelet风格的执行。

上述方法都缺乏统一的API和处理分布式并行计算的解决方案。HPX是C++库，优势是：编程API符合C++11/14标准，支持[远程操作]{.mark}。

与Charm++和X10等PGAS语言相比，HPX不使用新的语法和语义，HPX实施C++11的语法和语义，提供统一的API依赖于广泛接受的编程接口。因此，这有利于迁移legacy
code。

HPX可使用MPI作为可迁移的通信平台，同时HPX还可作为OpenMP和OpenCL的后端(back-end)，[便于迁移遗留代码]{.mark}。

## 3. HPX -- A GENERAL PURPOSE PARALLEL RUNTIME SYSTEM

SLOW

![](./media/image6.emf){width="4.166666666666667in"
height="1.4088188976377953in"}

### 3.1HPX的设计原则 {#hpx的设计原则 .标题3}

-   Focus on Latency Hiding instead of Latency Avoidance：MPI, GPGPU

-   Embrace Fine-grained Parallelism instead of Heavyweight Threads

-   Rediscover Constraint Based Synchronization to replace Global
    Barriers

-   Adaptive Locality Control instead of Static Data Distribution

-   Prefer MovingWork to the Data over Moving Data to theWork

-   Favor Message Driven Computation over Message Passing

### 3.2HPX的架构 {#hpx的架构 .标题3}

![](./media/image7.emf){width="3.739421478565179in" height="3.375in"}

### 3.3 The HPX API -- [Strictly Conforming to the C++ Standard]{.mark} {#the-hpx-api-strictly-conforming-to-the-c-standard .标题3}

HPX使用3种方式执行函数，见表1.

-   Synchronous function execution

-   Asynchronous function execution

-   Fire&Forget function execution

![](./media/image8.emf){width="5.768055555555556in"
height="3.132419072615923in"}

### 3.4可编程性 {#可编程性 .标题3}

HPX's governing principles, design, and high performance implementation
perfectly allow for nested parallelization by shifting the burden
imposed by writing [parallel codes from \"Joe the Scientist\" to our
expert \"Hero Programmer\".]{.mark}

## 4测试算例

### 4.2 Nbody code using LibGeoDecomp {#nbody-code-using-libgeodecomp .标题3}

![](./media/image9.emf){width="3.847288932633421in"
height="3.311895231846019in"}

![](./media/image10.emf){width="3.9619739720034994in"
height="3.400016404199475in"}

### [4.3 mini-Ghost]{.mark} {#mini-ghost .标题3}

Bulk Synchronous Parallel Programming Model (BSP)有限差分法求解PDE

convert the miniGhost applications BSP formulation to the
constraint-based dataflow style programming technique made available by
the HPX programming model. It combines the techniques developed in
[\[57\] and \[28\]]{.mark} to provide an implementation which removes
the bulk synchronous nature of algorithms needing halo-exchanges. It
also demonstrates a way to remove the global barrier imposed by the
global reduction operation needed for some of the variables by fully
overlapping computation with communication.

图7展示了使用OpenMP和HPX移植的[Mantevo
miniGhost]{.mark}应用。使用8核时，加速比为7.1，并行效率88%；使用16核时，由于增大的NUMA相关效应，加速比下降。由于这个原因，选择使用[每节点2进程]{.mark}的分布式运行。

![](./media/image11.emf){width="3.571499343832021in"
height="2.9597003499562553in"}

图8展示了分布式运行miniGhost的结果。[constraint-based并行化]{.mark}不仅对单节点有优势，而且由于使用了[统一的语法]{.mark}（见3.3节），对大规模分布式并行计算也是很有效的。

![](./media/image12.emf){width="3.813603455818023in"
height="3.265418853893263in"}

# Hartmut Kaiser et al. Higher-level Parallelization for Local and Distributed Asynchronous Task-Based Programming. 2015

## 1前言

E级超算下的可编程性与性能的迁移性。

在C++语言下整合各种并行化，如：iterative parallel execution, task-based
parallelism, asynchronous execution flows, continuation style
computation, and explicit fork-join control flow of independent and
non-homogeneous code paths.
这些高级抽象化似的C++代码可以应对日益增加的硬件架构复杂度。

syntactic and semantic

[on-node and off-node]{.mark} parallelism

HPX运行时系统，are fully aligned with modern C++ programming concepts,
are easily extensible, fully generic, and enable highly efficient
parallelization on par with or better than existing equivalent
applications based on OpenMP and/or MPI.

## 2相关研究

HPX与其他并行模型和运行时系统，如X10, Chapel,
Charm++,OpenMP和MPI的不同见[Kaiser et al. (2014)]{.mark}。

OpenMP (#pragma
omp)局限于循环结构外，在C++类型系统外执行，且容易放错位置。

Intel TBB和Microsoft Parallel Pattern Library不能应用于分布式内存场景。

## 3 HPX --通用目的的并行化C++运行时系统

HPX represents an innovative mixture of a global system-wide address
space (AGAS - Active Global Address Space), fine grain parallelism, and
lightweight synchronization combined with implicit, work queue based,
message driven computation, full semantic equivalence of local and
remote execution, and explicit support for hardware accelerators through
percolation.

HPX是第一个实施ParallelX执行模型开源的运行时系统。

HPXCL执行GPU异构并行计算的库。

APEX，一个policy
engine，利用HPX的效率counter框架，收集关于系统的信息，利用这些信息，并基于用于定义的policy，实施[运行时自适应(runtime-adaptive)的决策]{.mark}。

## 4并行化的基本概念

[work items, concurrent order]{.mark}

并行化执行：

（1）execution restrictions: 确保应用于work
items的线程安全（即可以并行运行，或者必须串行运行）；

（2）work items必须在哪个sequence中执行（即该work
items依赖于结果的获取性）；

（3）work items在何处执行（即\"on this core\", \"on the node\", \"on
this NUMA domain\" or \"wherever this data item is located\"等）；

（4）在相同执行线程上的运行任务的粒度大小控制（即\'执行的各线程应该运行相同数目的work
items\'）。

上述属性HPX定义为一个概念，见下表。某概念（实施一系列操作）对应（相同syntax
and semantics）的C++类型。

![](./media/image13.emf){width="3.3671467629046368in"
height="0.9990430883639545in"}

Parallelism TS

N4501: Working Draft: Technical Specification for C++ Extensions for
Concurrency. Technical report, , 2015.
http://www.openstd.org/jtc1/sc22/wg21/docs/papers/2015/n4501.html.

The C++ Standards Committee. ISO International Standard ISO/IEC
14882:2014, Programming Language C++. Technical report, Geneva,
Switzerland: International Organization for Standardization (ISO).,
2014. <http://www.open-std.org/jtc1/sc22/wg21>.

### 4.1 Execution Policies {#execution-policies .标题3}

[定义execution_policy]{.mark}：an object that expresses the requirements
on the ordering of functions invoked as a consequence of the invocation
of a standard algorithm

For example, calling an algorithm with a sequential execution policy
would require the algorithm to be run sequentially. Whereas calling an
algorithm with a parallel execution policy would permit, but not
require, the algorithm to be run in parallel.

Parallelism
TS定义了三种execution_policy，允许显式添加各自的执行（见下表，HPX实施的所有执行策略）。

HPX中增加了par(task)和seq(task)这两个task执行策略，任务执行策略植入算法后立即返回，返回到future对象的激活位置，表征算法的最终结果。例如，带par计算的all_of算法返回bollean结果；而调用带par(task)的all_of算法立即返回一个future对象表征一个forthcoming
boolean。任务执行策略也可以整合并行算法和异步执行流的fork-join task
blocks（[见5.2节]{.mark}）。

![](./media/image14.emf){width="4.000120297462817in"
height="2.3835640857392826in"}

除了可以选择创建任务执行策略，每个HPX执行策略还有相关的默认[executor和executor_parameter]{.mark}实例，可通过execution_policy接口访问该实例。另外，执行策略还可以再绑定到另一个executor或executor参数对象（[见列表1的实例代码]{.mark}）。

![](./media/image15.emf){width="3.661558398950131in"
height="2.9671806649168855in"}

### 4.2 Executors {#executors .标题3}

定义："an object responsible for creating execution agents on which work
is performed, thus abstracting the (potentially platform-specific)
mechanisms for launching work".

HPX依靠executor_traits利用executors，一个executor实施async_execute，返回一个future
([见5.1节]{.mark})对象，表示一个异步函数激活的结果。从这个实例，同步的execute可以由executor_traits合成，或者如果实施，提交给executor

HPX还有其他的扩展。。。

HPX将不同的预定义的executor类型暴露给用户。除了一般的串行和并行executor，用作默认为seq和par执行策略，HPX实施executor封装特殊的schedulers，像NUMA-aware
schedulers, LIFO or FIFO scheduling policy schedulers, or executors to
manage distributed work.

### 4.3 Executor Parameters {#executor-parameters .标题3}

HPX中增加了executor_parameters的概念，允许控制工作的粒度，即：相同的执行线程（agent）上执行哪个以及多少work
items。这与OpenMP
static或guided调度原语很像。但OpenMP调度原语不是C++类型，HPX属于executor_parameters的类型可以在运行时做出决策。

在运行时决策的情况，executor_parameters还允许定义某并行操作可以使用多少处理单元（核心）。

用户可以改造executor_parameters，适应特殊的应用。

## 5并行化的基本类型

各种类型的并行化（列表1）可通过HPX
API使用，可理解为抽象层，在各自顶部实施。[图1]{.mark}展示了实施层的示意图。应用通过调用显示的层直接访问所有类型的并行化。HPX还可实施运行时自适应的执行，如预定义的executor
parameters类型[auto_chunk_size]{.mark}，可运行时自适应地决定在相同线程上执行多个步并行算法的迭代。

![](./media/image16.emf){width="3.9722222222222223in"
height="3.873681102362205in"}

### 5.1 Task-based Parallelism {#task-based-parallelism .标题3}

基于任务的并行是最底层的API。HPX中基于任务并行的主要方法是[async]{.mark}模板函数，可接收一个函数和形参。async设计用来调度[不同长度和时间]{.mark}的任务的并行执行，async调度某函数\'as
if on a new
threads，立即返回一个future对象，表示调度函数的结果。使用async，可实现本地和远程异步函数的激活。

async返回的future对象自然建立了显式的数据依赖，作为随后操作依赖该future表示的结果，可异步执行（[见5.2节]{.mark}）。

### 5.2 Asynchronous Flow Control {#asynchronous-flow-control .标题3}

HPX还可以[串行或并行地组合]{.mark}使用future：

（1）通过调用future的成员函数f.then(g)实现[sequential
composition]{.mark}，f.then(g)将某函数g依附到future对象f，这里成员函数返回一个新的future对象，表示依附的继续执行函数g的结果。一旦future
f准备好后，函数g就异步激活了。[sequential
composition]{.mark}是串行执行若干tasks的主要机制，可与其他tasks并行执行。

（2）实施Parallel composition：使用工具函数when_all(f1, f2,
...)，也返回一个future对象。一旦所有形参futures
f1,f2,...准备好后，就准备好一个返回的future对象。Parallel
composition主要是为fork-join风格的任务执行构建blocks，其中有若干任务并行执行，调度[以上任务]{.mark}必须在[其他任务]{.mark}之前完成。还有其他工具补充API，如when_any或when_some等待一个或若干futures准备好。

[dataflow函数]{.mark}整合了Sequential composition和Parallel
composition，是async的特殊版本，直到所有future形参都准备好了，才延迟执行传递的函数。[表3]{.mark}展示了[使用dataflow]{.mark}，确保gather_async的所有结果都在两个partition步骤之后计算。

![](./media/image17.emf){width="3.6613167104111985in"
height="3.1378094925634294in"}

HPX中的任何异步操作生成的future对象，不管是该操作是local还是remote（[交错执行？]{.mark}），都是可用的。也就是说：支持API层级上计算与通信的重叠，无需开发者额外的编程。

### 5.3 Loop-based Parallelism {#loop-based-parallelism .标题3}

HPX实施几乎所有的C++
STL算法的并行，除了要求第一个形参要求是一个execution
policy。HPX并行算扩展后支持各种类型的执行策略（包括异步执行），联合全套的executor和executor
parameters（见[第4节）]{.mark}，用户可使用API开发应用程序。

6.1节展示了改造的NUMA-aware的executor的benchmark。

[列表3]{.mark}展示了激活一种并行算法,
[stable_partition]{.mark}，使用[异步执行策略]{.mark}。

![](./media/image18.emf){width="3.892302055993001in"
height="2.5971227034120736in"}

HPX并行算法通常在[locality]{.mark}执行循环体（迭代），locality对应data
item的位置。对于[纯局部数据结构（即std:vector）]{.mark}，所有的执行都是局部的；而对于[分布式数据结构（即hpx::partitioned_vector）]{.mark}，每个特殊的迭代都是紧密结合data
item来执行的，即在正确的locality处。特殊的executor，基于HPX的分distribution
policy，构建[一种抽象化数据布置和在不同的locality的分布的概念]{.mark}，可以更精细地控制这个映射过程，比如启动SPMD风格（[多线程并行？]{.mark}）的执行，[各locality]{.mark}仅对分布数据的[local
portion]{.mark}执行操作。

### 5.4 Fork-join Parallelism {#fork-join-parallelism .标题3}

C++并行算法基于[task_group]{.mark}概念建立了define_task_block函数，是PPL和TBB库的子集。C++并行算法没有整合执行策略和executor的概念进入fork-join并行。

HPX扩展功能，可传递[execution policy]{.mark}到[task
block]{.mark}。这样，就全面支持executor，在何处可应用executor
parameters。对于task
block，使用异步执行策略是很有效的算法，很容易整合异步执行流。

## 6 Benchmark

### 6.1 The STREAM Benchmark {#the-stream-benchmark .标题3}

单节点上的纯局部并行效率

标准的STREAM测试算例(OpenMP版本)以及HPX开发的版本（[代码见HPX源码内]{.mark}）

测试机器：Intel CPU，10核，没有使用超线程；编译器：Intel C++

迭代10步

高度优化的OpenMP应用，HPX移植版本，有3%的性能损失，如图2.

为测量系统的内存带宽，创建的OpenMP线程仅访问位于与运行线程的NUMA域相关的内存bank内的数据。

[OpenMP实现方法]{.mark}：在分配内存(malloc())后使用NUMA placement
policy，即：第一次输出到新分配内存的线程决定了locality
domain，其中在分配虚拟地址后[放置]{.mark}(place)映射的物理地址（first
touch
policy）。如果初始化数组的循环以相同的并行化方式运行，如同循环使用数据那样，使用相同的访问模式，[cross-NUMA域]{.mark}的数据转移最小化。这种first
touch初始化的预处理，保证所有线程在整合运行期间对相同核心的亲和性。这样就降低了系统噪音，通过避免跨NUMA域的通信，最大化内存和内核间的数据转移。

对2个循环使用#pragma omp parallel
static原语，保证初始化和实际测量期间的访问模式的一致性。同时，使用外部OS工具绑定线程（如numactl
or likwid-pin），OpenMP本身不提供这些工具。

![](./media/image19.emf){width="4.028353018372703in"
height="3.933857174103237in"}

原始的STREAM算例由4个并行循环，其中3个循环是独立的，相同大小数组(a,b,c)：copy
(c=a), scale (b=k\*c), add (c=a+b)，和一个triad步(a=b+k\*c)

HPX版本算例，使用对应的并行算法，替换并行OpenMP循环（#pragma原语），实施相同操作。使用copy,
unary and binary transform并行算法（见[表4]{.mark}）。

![](./media/image20.emf){width="3.826388888888889in"
height="3.937678258967629in"}

### 6.2 Matrix Transposition {#matrix-transposition .标题3}

矩阵转置的真实mini应用，使用多种类型的并行化。

3个原始版本：纯[OpenMP, MPI (one MPI rank per core)和MPI+OpenMP]{.mark}

HPX源码中包含该算例的HPX迁移版本，使用了asynchronous, continuation based
constructs, execution policies and executors, and parallel algorithms

图3展示了block矩阵的转移算法。

OpenMP和HPX版本的算例，都使用first-touch数据定位（HPX通过NUMA-aware
allocator）。操作都是对相同数据块使用相同线程，因此最小化cross-NUMA-domain内存通信，改善数据和高速缓存局部性。

HPX版本：各block转置操作是在NUMA-domain上运行，其中安置目标block，源block可以是local或remote，两者的NUMA-domain内存访问都接近1

![](./media/image21.emf){width="3.6666666666666665in"
height="3.476519028871391in"}

HPX与MPI+OpenMP算例比较，对于weak-scaling测试，产生的网络信息数目越大，整体效率越低，还需要[调整HPX网络]{.mark}缓解该问题。显然效率降低不是高层级抽象引起的。

![](./media/image22.emf){width="3.5215135608048995in"
height="3.358509405074366in"}

## 参考文献

H. Kaiser, T. Heller, B. Adelstein-Lelbach, A. Serio, and D. Fey. [HPX:
A Task Based Programming Model in a Global Address Space]{.mark}. In
Proceedings of the 8th International Conference on Partitioned Global
Address Space Programming Models, PGAS '14, pages 6:1-6:11, New York,
NY, USA, 2014. ACM.

H. Kaiser, M. Brodowicz, and T. Sterling. ParalleX: An Advanced Parallel
Execution Model for Scaling-Impaired Applications. In Parallel
Processing Workshops, pages 394-401, Los Alamitos, CA, USA, 2009. IEEE
Computer Society.

# HPX的应用测试和效率评估

## HPX自带example codes

1d_stencil

(Matrix) Transposition

## Benchmark

Fibonacci

mini-Ghost

STREAM

Octopus: an HPX octree-based 3D AMR framework

## 参考文献

Alice Koniges, et al. HPX Applications and Performance Adaptation

# HPX在ARM机器上的测试

HPX在3种ARM集群上做了测试，包括：ThunderX2 (Marvell, USA), Kunpeng 916
(Huawei, China)和A64FX (Fujitsu, Japan)

## STREAM COPY

测试Memory copy bandwidth

![](./media/image23.emf){width="3.087071303587052in"
height="4.307104111986002in"}

## 1D Stencil算例

![](./media/image24.emf){width="3.2044663167104113in"
height="3.6946708223972005in"}

## 2D Stencil

![](./media/image25.emf){width="4.805555555555555in"
height="2.8877088801399826in"}

![](./media/image26.emf){width="4.77328302712161in"
height="3.141451224846894in"}

![](./media/image27.emf){width="2.5829308836395453in"
height="3.5202307524059493in"}

## 结论

We demonstrate that the application scales both on-node and distributed.

We found that performance on Arm processors is as good or better than
their x86 brethren. For the 1D stencil, all processors except Kunpeng
916 showed good scaling results. In the case of Kunpeng 916, the poor
interconnect network is to be blamed.

For the 2D stencil, we observed that processors with large cache lines
showed inherent cache blocking benefits (without explicit
implementation). This resulted in about a 50% performance boost over the
expected results. We also observed that explicit vectorization can
improve the performance significantly for ThunderX2 and Kunpeng 916 due
to considerably lower CPU stall counts when compared with auto
vectorized codes. For A64FX, we did not observe any visible performance
benefits by employing explicit vectorization.

### 参考文献 {#参考文献-2 .标题3}

Performance Evaluation of ParalleX Execution model on Arm-based
Platforms
