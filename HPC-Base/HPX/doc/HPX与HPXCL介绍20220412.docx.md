# HPX介绍:

# Kaiser et al., (2020). HPX - The C++ Standard Library for Parallelism and Concurrency. Journal of Open Source Software, 5(53), 2352. https://doi.org/10.21105/joss.02352

## [Asynchronous many-task systems (AMT)]{.mark}

HPX is a C++ Library for concurrency and parallelism that is developed
by The STE\|\|AR Group. HPX exposes extended services and
functionalities supporting the implementation of parallel, concurrent,
and distributed capabilities for applications in any domain; it has been
used in scientific computing, gaming, finances, data mining, and other
fields.

## HPX目的

HPX AMT运行时系统，着眼提高并行效率和高度利用计算资源：

（1）HPX exposes a C++ standards conforming API that unifies syntax and
semantics for local and remote operations.
简化编程，以协调方式运行在各种并行机器上(i.e., on-node, off-node, and
accelerator-based parallelism)；

（2）HPX实施异步的C++标准编程模型，可[半自动并行化]{.mark}用户代码。HPX的API可实施[计算与通信的内部重叠]{.mark}，prefers
moving work to data over moving data to work, and exposes minimal
overheads from its lightweight threading subsystem, ensuring efficient
fine-grained parallelization and minimal-overhead synchronization and
context switching.

HPX的主要目的是通过增加资源利用改进并行程序的效率和scalability，以及通过异步API和实施adaptive
scheduling降低同步成本。

HPX有序使用future实施计算与通信的重叠和基于约束的同步。

HPX is able to maintain a balanced load among all the available
resources resulting in significantly reducing processor starvation and
effective latencies while controlling overheads.

HPX uses the concept of C++ [Futures]{.mark} to transform sequential
algorithms into wait-free asynchronous executions. The use of
Futurization enables the automatic creation of dynamic data flow
execution trees of potentially millions of lightweight HPX tasks
executed in the proper order.

HPX also provides a work-stealing task scheduler that takes care of
fine-grained parallelizations and automatic load balancing.

## HPX组成

-   Threading
    > Subsystem：轻量级线程管理器，降低不同线程协调执行时的同步成本。Work-stealing
    > and work-sharing policies实现任务的自动局部荷载均衡。

-   Active Global Address Space (AGAS)：通过对象迁移，实现荷载均衡。

-   Parcel Transport Layer：active-message networking layer.
    > Additionally, its asynchronous protocol enables the parcelport to
    > implicitly overlap communication and computation.
    > 默认HPX支持TCP/IP, MPI和[libfabric]{.mark}。

-   Performance counters：现场实施监控效率。

-   Policy Engine/Policies：Autonomic Performance Environment for
    > Exascale
    > (APEX)实现程序适应运行时环境，通过度量HPX任务增加灵活性。parcel
    > coalescing特性适应一个应用的当前阶段。

-   Accelerator Support：整合GPU计算，有2中： HPXCL和HPX.Compute
    > (SYCL)。整合HPX与Kokkos在进行中。

-   Local Control Objects (synchronization support
    > facilities)：支持C++20原语，对于异步计算，HPX提供hpx::async and
    > hpx::future

-   Software Resilience：软件恢复

-   C++ Standards conforming
    > API：HPX实施C++17并行算法，拓展为异步版本，例如：HPX提供hpx::execution::seq
    > and hpx::execution::par execution policies及他们的asynchronous
    > equivalents: hpx:: execution:: seq (hpx::execution::task) and
    > hpx::execution::par(hpx::execution::task)，参考Octo-Tiger。HPX还实施了C++20的并发API：hpx::jthread,
    > hpx::latch, hpx::barrier, etc.

![](./media/image1.emf)

## 应用

Octo-Tiger：天文学计算(AMR技术)

libGeoDecomp：加速基于模板的代码自动并行的库；

NBody Problem: 基于libGeoDecomp开发

## 示例代码

（1）使用C++17标准定义的execution
policies，实施HPX的并行算法API。并行算法通过增加一个形参（称为[execution
policy]{.mark}），拓展经典的STL算法。

hpx::execution::seq执行串行计算；hpx::execution::par执行并行计算。

![](./media/image2.emf)

![](./media/image3.emf)

（2）计算*sin*(*x*)的泰勒级数![](./media/image4.emf)。区间\[0, N\]分成2部分\[0,
N/2\]和\[N/2+1, N\]，使用hpx::async异步计算。

[注意]{.mark}：各异步函数调用返回一个hpx::future，用来同步收集部分计算结果。future有一个get()方法，一旦泰勒级数计算完成后，返回计算结果。如果结果没准备好，暂停当前线程直到结果准备好。仅当f1和f2准备好后，将打印整体结果到标准输出数据流。

![](./media/image5.emf)

![](./media/image6.emf)

# HPX与CUDA的整合(HPXCL)

HPXCL着眼于整合已有的CUDA核函数与HPX的异步执行图。

HPX.Compute着眼于隐藏CUDA特殊语言特征。

HPX.Compute SYCL利用SYCL作为新的后端。

HPXCL是HPX的拓展，拓展了AMT编程方法，实现host到device的异步数据转移（反之亦然）以及异步的核函数执行。基于future的概念，实现CUDA设备与CPU间任务的同步。

HPX主要使用[3个组件]{.mark}解决远程任务：

-   Thread Manager

-   Active Global Address Space (AGAS) Service

-   Parcel Service

![](./media/image7.emf)

## 1 Futurization

HPXCL
API完全是异步的，返回hpx::future，一个统一的异步返回值，Percolation与任何以HPX编码的应用程序紧密整合。通过使用标准协调的API函数，如hpx::future\<T\>::then和hpx::when_all\<T\>（用于composition）以及hpx::dataflow（构建隐式的并行执行流图），编写futurized代码。

## 2 设计与实施

[图2]{.mark}显示了类之间的基本结构和关系。

如果为某设备创建一个buffer或kernel，client side
object仅引用实际内存。一旦使用这些objects，操作对加速器完全是局部的，相关kernel对数据执行。待执行的设备代码just-in-time编译。这是实施percolation，允许数据和代码在分布式系统内自由移动（使用parcel
service）。使用TCP或MPI在计算节点间移动数据。

device\-\--加速器的逻辑表征，定义待执行核函数的功能，创建内存缓存，实施同步。HPXCL暴露系统中局部和远程设备的功能。

buffer---表示在指定device上待分配内存。buffer上的操作与device from and
to
host以及不同设备间的复制数据有关。异步copy允许有效利用不同实体间的内存交换。copy函数返回future，可用于对kernel调用的依赖，还允许通信与计算的自然重叠。

program---在指定设备上待执行的代码。通过执行核函数，需要提供buffer作为形参。执行一个核函数也会返回一个future对象。这些future也可用来表述来自内存拷贝操作的数据流依赖，或者其他之前对kernel的调用。

![](./media/image8.emf)

图2 Class diagram of Buffer, Device and Program and the functionality
provided by each class. The device represents a logical device on a
remote or local locality. A program which handles the compilation and
the potential remote launches of kernels. The memory of a device is
represented by a buffer.

### 2.1访问当地和远程设备

[Listing
1]{.mark}显示了代码的第1行，发现当地和远程设备。方法get_all_devices有2个形参(major
and minor compute
capability)，返回std::vector，包含所有可用的设备hpx::cuda::device，至少具备指定的计算能力。方法返回一个future，因此必须调用.get()接收该future的内存。注意：所有设备对象有相同的API，与设备是当地或远程的无关。

Listing 1

### 2.2 HPXCL的工作流程

Listing 2列出了运行CUDA核函数的工作流程，用来计算N个元素的和。

注意到：我们使用native
CUDA功能同步异步CUDA函数调用，但通过返回一个future对象隐藏了用户层的操作。这允许用户在集群环境下以统一的方式整合CPU-GPU上的任务(tasks-with-tasks)。另外，远程和当地设备的使用具有相同的统一API，HPXCL内部复制数据到节点（需要数据的地方）。

## 3开销

统一的API和使用HPX会引入额外开销(overhead)。还使用native
CUDA测试了benchmark。

### 3.1单个设备

Nvidia Tesla K40和Nvidia Tesla K80安装在不同计算节点(bahram,
reno)上。每个benchmark使用相同的核函数，使用native CUDA与HPX执行。

[3.1.1 Stencil Kernel]{.mark}

3-点stencil: s(*xi*) = 0:5*xi-*1 +*xi*+0.5*xi+*1

block_size = 1; thread_size = 32

HPX实施约比native
CUDA实施快28%，重叠计算和数据转移，降低了整体计算耗时。这里CUDA代码顺序执行，HPX代码使用CUDA
SDK的异步功能。

![](./media/image9.emf)

图3

[3.1.2 Partition example]

native CUDA的实施代码：M. Harris, How to Overlap Data Transfers in CUDA
C/C++. 2012.
https://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/

2种实施都使用了异步函数。

核函数：$k\left( x_{i} \right): = \sqrt{\sin^{2}i + \cos^{2}(i)}$，$X: = \left\{ x_{i},\ldots,X_{n} \mid x_{i}\mathbb{\in R} \right\}$

向量长度$n = 2^{m}*1024*blockSize*p$.其中, *m*={1,2,...,7,8},
blockSize=256, 分区数目*p*=4

向量分*p*个分区，各分区异步拷贝到CUDA设备，执行第*k*个核函数，然后结果异步拷贝返回主机，见[算法1]{.mark}。native
CUDA和HPX使用CUDA流实施同步。

![](./media/image10.emf)

HPX实施比native CUDA实施快4%。

HPXCL引入的开销在向量很大时可忽略。

![](./media/image11.emf)

图4

[3.1.3 Mandelbrot（使用与CPU的concurrency）]{.mark}

使用Mandelbrot数据集增加图像大小（使用HPXCL），以PNG图像格式保存到文件系统。

图5中：蓝线是使用同步方式的计算和写图像的计算耗时；黑线是使用HPXCL的与CPU的concurrency的方式。

![](./media/image12.emf)

图5

HPXCL[新开发的]benckmarks:

\(1\) 稠密矩阵相乘(dgemm) (CUDA and HPXCL versions)；

\(2\) 稀疏矩阵向量乘积(smvp) (CUDA and HPXCL versions)；

(3)stencil: 包含了上面的partition和stencil的2个算例(CUDA and HPXCL
versions)；

\(4\) stream: 基于HPX的STREAM算例(CUDA and HPXCL versions)。

exapmpes包括演示：

(1)如何从字符串(核函数复杂时不要用这种方式)和从文件（建议）编译CUDA函数；

(2)使用共享内存(shared memory)；

(3)p2p方式的多GPU设备copy数据；

(4)使用多CUDA流(Streams)实施并发的(concurrent)的通信与计算重叠技术。

### 3.2多设备

一个计算节点上有2个*Tesla* K80卡。注意：各K80卡有dual-GPU设计，因此有2×2
GPUs。

3.2.1 Partition example

3.1.2节的4个分区由K80卡中的一个来计算。图6中黑线显示native
CUDA执行时间；蓝线显示1到4个
K80设备的执行时间。两种情况使用多个设备时，执行时间都增加。但执行时间相差1个量级。多个GPU设备的情况，HPXCL引入的overhead较小，执行时间更快。

![](./media/image13.emf)

图6

## 4结论

基于3个benchmark，比较了native
CUDA和HPXCL实施的计算效率。对公平对比，对同步(synchronization)使用相同CUDA特性，如CUDA流、异步内存功能(cudaMemcpyAsync)和cudaStreamSynchronize。但是HPX使用轻量级线程（甚至使用一个CPU），产生多个轻量级线程。因此，HPX受益于使用多线程，会有更快的计算效率。

CUDA核函数在运行时使用NVRTC-CUDA runtime编译，而native
CUDA应用是在compile time编译。这就很难公平对比执行时间了。

目前的抽象化改善了可编程性和异构的分布式workloads的维护。CPU上的数据转移和核函数启动很容易整合在异构workloads中，例如Mandelbrot
benchmark中输出图像，同时计算下一帧图像大小。

[展望]：2个HPX应用：PeridynamicHPX和nast_hpx，迁移到HPXCL。

大多数[CFD问题]，matrix-vector操作是瓶颈；而PeridynamicHPX中，临近搜索是瓶颈。这些任务可以在GPU上执行，获取快速算法。

## 参考文献

Kaiser et al., 2020. HPX - The C++ Standard Library for Parallelism and
Concurrency. Journal of Open Source Software, 5(53), 2352.

Patrick Diehl, Madhavan Seshadri, Thomas Heller, Hartmut Kaiser. 2018.
Integration of CUDA Processing within the C++ library for parallelism
and concurrency (HPX). arXiv:1810.11482v1
