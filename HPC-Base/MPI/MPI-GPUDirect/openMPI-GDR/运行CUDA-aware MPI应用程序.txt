Open MPI v2.0.0 New Features
CUDA support through UCX
Improved on-node Host to GPU transfers using gdrcopy for improved Send/Recv performance.

1 查看构建的MPI库是否具备了CUDA-aware支持?
shell$ ompi_info --parsable --all | grep
mpi_built_with_cuda_support:value

屏幕显示：
mca:mpi:base:param:mpi_built_with_cuda_support:value:true
表示：支持!

使用 verbose flags，获取更多信息. opal_cuda_verbose参数仅有1层verbosity
shell$ mpirun --mca opal_cuda_verbose 10 ...

mpi_common_cuda_verbose参数提供CUDA-aware相关活动的更多信息。除非有特殊问题，一般不使用该参数。
shell$ mpirun --mca mpi_common_cuda_verbose 10 ...
shell$ mpirun --mca mpi_common_cuda_verbose 20 ...
shell$ mpirun --mca mpi_common_cuda_verbose 100 ...

CUDA IPC使用的相关MCA参数(open MPI v1.7.4引入)，可以关闭：
mpirun --mca btl_smcuda_use_cuda_ipc 0 ...

In addition, it is assumed that CUDA IPC is possible when running on the same GPU, and this is typically true. However, there is the ability to turn it off.

shell$ mpirun --mca btl_smcuda_use_cuda_ipc_same_gpu 0 ...

Last, to get some insight into whether CUDA IPC is being used, you can turn on some verbosity that shows whether CUDA IPC gets enabled between two GPUs.

shell$ mpirun --mca btl_smcuda_cuda_ipc_verbose 100 ...


2 Open MPI v1.7.4开始，增加了支持基于Mellanox NIC的GPUDirect RDMA功能。

2.1 查看MPI库是否具备支持GPUDirect RDMA功能？
shell$ ompi_info --all | grep btl_openib_have_cuda_gdr

屏幕显示：
MCA btl: informational "btl_openib_have_cuda_gdr" (current value: "true", data source: default, level: 4 tuner/basic, type: bool)
表示：支持!
   
2.2 查看OFED栈是否支持GPUDirect RDMA功能？
shell$ ompi_info --all | grep btl_openib_have_driver_gdr

屏幕显示：
MCA btl: informational "btl_openib_have_driver_gdr" (current value: "true", data source: default, level: 4 tuner/basic, type: bool) 
表示：支持!

2.3 现在，可以运行GPUDirect RDMA功能的应用程序
mpirun --mca btl_openib_want_cuda_gdr 1 ...


3 GPUDirect RDMA实施细节
With GPUDirect RDMA support selected, the eager protocol is unused. This is done to avoid the penalty of copying unexpected GPU messages into host memory. Instead, a rendezvous protocol is used where the sender and receiver both register their GPU buffers and make use of GPUDirect RDMA support to transfer the data. This is done for all messages that are less than 30,000 bytes in size. For larger messages, the openib BTL switches to using pipelined buffers as that has better performance at larger message sizes. So, by default, with GPUDirect RDMA enabled, the underlying protocol usage is like this:
传输的信息超过30,000 bytes后，使用异步拷贝
0      < message size < 30,000      GPUDirect RDMA
30,000 < message size < infinity    Asynchronous copies through host memory
   
可以使用 --mca btl_openib_cuda_rdma_limit ，手动调节切换至异步拷贝的字节阈值：
增加至100000 bytes：
mpirun --mca btl_openib_cuda_rdma_limit 100000 ...
 
NUMA问题：如果一个计算节点上有多个GPU，打算选择最靠近运行进程的GPU，可利用hwloc库。下面的示例代码展示，如果决定在哪个CPU上运行，然后寻找最靠近的GPU。可能有相同距离的多个GPU，这依赖于hwloc在系统上的位置。
/**
 * Test program to show the use of hwloc to select the GPU closest to the CPU
 * that the MPI program is running on.  Note that this works even without
 * any libpciacces or libpci support as it keys off the NVIDIA vendor ID.
 * There may be other ways to implement this but this is one way.
 * January 10, 2014
 */
#include <assert.h>
#include <stdio.h>
#include "cuda.h"
#include "mpi.h"
#include "hwloc.h"
 
#define ABORT_ON_ERROR(func)                          \
  { CUresult res;                                     \
    res = func;                                       \
    if (CUDA_SUCCESS != res) {                        \
        printf("%s returned error=%d\n", #func, res); \
        abort();                                      \
    }                                                 \
  }
static hwloc_topology_t topology = NULL;
static int gpuIndex = 0;
static hwloc_obj_t gpus[16] = {0};
 
/**
 * This function searches for all the GPUs that are hanging off a NUMA
 * node.  It walks through each of the PCI devices and looks for ones
 * with the NVIDIA vendor ID.  It then stores them into an array.
 * Note that there can be more than one GPU on the NUMA node.
 */
 
static void find_gpus(hwloc_topology_t topology, hwloc_obj_t parent, hwloc_obj_t child) {
    hwloc_obj_t pcidev;
    pcidev = hwloc_get_next_child(topology, parent, child);
    if (NULL == pcidev) {
        return;
    } else if (0 != pcidev->arity) {
        /* This device has children so need to look recursively at them */
        find_gpus(topology, pcidev, NULL);
        find_gpus(topology, parent, pcidev);
    } else {
        if (pcidev->attr->pcidev.vendor_id == 0x10de) {
            gpus[gpuIndex++] = pcidev;
        }
        find_gpus(topology, parent, pcidev);
    }
}
int main(int argc, char *argv[])
{
    int rank, retval, length;
    char procname[MPI_MAX_PROCESSOR_NAME+1];
    const unsigned long flags = HWLOC_TOPOLOGY_FLAG_IO_DEVICES | HWLOC_TOPOLOGY_FLAG_IO_BRIDGES;
    hwloc_cpuset_t newset;
    hwloc_obj_t node, bridge;
    char pciBusId[16];
    CUdevice dev;
    char devName[256];
 
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if (MPI_SUCCESS != MPI_Get_processor_name(procname, &length)) {
        strcpy(procname, "unknown");
    }
 
    /* Now decide which GPU to pick.  This requires hwloc to work properly.
     * We first see which CPU we are bound to, then try and find a GPU nearby.
     */
    retval = hwloc_topology_init(&topology);
    assert(retval == 0);
    retval = hwloc_topology_set_flags(topology, flags);
    assert(retval == 0);
    retval = hwloc_topology_load(topology);
    assert(retval == 0);
    newset = hwloc_bitmap_alloc();
    retval = hwloc_get_last_cpu_location(topology, newset, 0);
    assert(retval == 0);
 
    /* Get the object that contains the cpuset */
    node = hwloc_get_first_largest_obj_inside_cpuset(topology, newset);
 
    /* Climb up from that object until we find the HWLOC_OBJ_NODE */
    while (node->type != HWLOC_OBJ_NODE) {
        node = node->parent;
    }
 
    /* Now look for the HWLOC_OBJ_BRIDGE.  All PCI busses hanging off the
     * node will have one of these */
    bridge = hwloc_get_next_child(topology, node, NULL);
    while (bridge->type != HWLOC_OBJ_BRIDGE) {
        bridge = hwloc_get_next_child(topology, node, bridge);
    }
 
    /* Now find all the GPUs on this NUMA node and put them into an array */
    find_gpus(topology, bridge, NULL);
 
    ABORT_ON_ERROR(cuInit(0));
    /* Now select the first GPU that we find */
    if (gpus[0] == 0) {
        printf("No GPU found\n");
        exit(1);
    } else {
        sprintf(pciBusId, "%.2x:%.2x:%.2x.%x", gpus[0]->attr->pcidev.domain, gpus[0]->attr->pcidev.bus,
        gpus[0]->attr->pcidev.dev, gpus[0]->attr->pcidev.func);
        ABORT_ON_ERROR(cuDeviceGetByPCIBusId(&dev, pciBusId));
        ABORT_ON_ERROR(cuDeviceGetName(devName, 256, dev));
        printf("rank=%d (%s): Selected GPU=%s, name=%s\n", rank, procname, pciBusId, devName);
    }
 
    MPI_Finalize();
    return 0;
} 
 
 
 
 