2. How do I develop CUDA-aware Open MPI applications?

Developing CUDA-aware applications is a complex topic, and beyond the scope of this document. CUDA-aware applications often have to take machine-specific considerations into account, including the number of GPUs installed on each node and how the GPUs are connected to the CPUs and to each other. Often, when using a particular transport layer (such as OPA/PSM2) there will be run-time decisions to make about which CPU cores will be used with which GPUs.

A good place to start is the nVidia CUDA Toolkit Documentation, including the Programming Guide and the Best Practices Guide. For examples of how to write CUDA-aware MPI applications, the nVidia developers blog offers examples and the OSU Micro-Benchmarks offer an excellent example of how to write CUDA-aware MPI applications.



3. Which MPI APIs work with CUDA-aware?
MPI_Send, MPI_Bsend, MPI_Ssend, MPI_Rsend, MPI_Isend, MPI_Ibsend, MPI_Issend, MPI_Irsend, MPI_Send_init, MPI_Bsend_init, MPI_Ssend_init, MPI_Rsend_init, MPI_Recv, MPI_Irecv, MPI_Recv_init, MPI_Sendrecv, MPI_Bcast, MPI_Gather, MPI_Gatherv, MPI_Allgather, MPI_Allgatherv, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Scatter, MPI_Scatterv	Open MPI v1.7.0
MPI_Win_create, MPI_Put, MPI_Get	Open MPI v1.8.0
MPI_Reduce, MPI_Allreduce, MPI_Scan, MPI_Exscan, MPI_Reduce_scatter, MPI_Reduce_scatter_block	Open MPI v1.8.5


5. How do I use CUDA-aware UCX for Open MPI?
Example of running osu_latency from OSU benchmarks with CUDA buffers using Open MPI and UCX CUDA support:

mpirun -np 2 --mca pml ucx -x UCX_TLS=rc,sm,cuda_copy,gdr_copy,cuda_ipc ./osu_latency D D

6. Which MPI APIs work with CUDA-aware UCX?
MPI_Send, MPI_Bsend, MPI_Ssend, MPI_Rsend, MPI_Isend, MPI_Ibsend, MPI_Issend, MPI_Irsend, MPI_Send_init, MPI_Bsend_init, MPI_Ssend_init, MPI_Rsend_init, MPI_Recv, MPI_Irecv, MPI_Recv_init, MPI_Sendrecv, MPI_Bcast, MPI_Gather, MPI_Gatherv, MPI_Allgather, MPI_Reduce, MPI_Reduce_scatter, MPI_Reduce_scatter_block, MPI_Allreduce, MPI_Scan, MPI_Exscan, MPI_Allgatherv, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Scatter, MPI_Scatterv, MPI_Iallgather, MPI_Iallgatherv, MPI_Ialltoall, MPI_Iialltoallv, MPI_Ialltoallw, MPI_Ibcast, MPI_Iexscan

8. Can I tell at compile time or runtime whether I have CUDA-aware support?
include mpi-ext.h

/*
 * Program that shows the use of CUDA-aware macro and runtime check.
 * Requires Open MPI v2.0.0 or later.
 */
#include <stdio.h>
#include "mpi.h"
 
#ifdef
#include "mpi-ext.h" /* Needed for CUDA-aware check */
#endif
 
int main(int argc, char *argv[])
{
    printf("Compile time check:\n");
#if defined(MPIX_CUDA_AWARE_SUPPORT) && MPIX_CUDA_AWARE_SUPPORT
    printf("This MPI library has CUDA-aware support.\n", MPIX_CUDA_AWARE_SUPPORT);
#elif defined(MPIX_CUDA_AWARE_SUPPORT) && !MPIX_CUDA_AWARE_SUPPORT
    printf("This MPI library does not have CUDA-aware support.\n");
#else
    printf("This MPI library cannot determine if there is CUDA-aware support.\n");
#endif /* MPIX_CUDA_AWARE_SUPPORT */
 
    printf("Run time check:\n");
#if defined(MPIX_CUDA_AWARE_SUPPORT)
    if (1 == MPIX_Query_cuda_support()) {
        printf("This MPI library has CUDA-aware support.\n");
    } else {
        printf("This MPI library does not have CUDA-aware support.\n");
    }
#else /* !defined(MPIX_CUDA_AWARE_SUPPORT) */
    printf("This MPI library cannot determine if there is CUDA-aware support.\n");
#endif /* MPIX_CUDA_AWARE_SUPPORT */
 
    return 0;
}
















